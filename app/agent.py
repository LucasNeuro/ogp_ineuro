from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from pydantic_ai import Agent, Tool
from pydantic_ai.mcp import MCPServerHTTP
from dotenv import load_dotenv
from .settings import get_settings
from .llm_router import llm_router
import os
import json

load_dotenv()
settings = get_settings()

class MCPServer(BaseModel):
    """Modelo para configura√ß√£o de servidor MCP"""
    name: str
    url: str
    api_key: str
    description: Optional[str] = None

class INeuroAgent:
    """Agente I-Neuro com suporte a MCP"""
    
    def __init__(self):
        self.name = "I-Neuro"
        self.description = "Agente de atendimento inteligente com suporte a m√∫ltiplas ferramentas via MCP"
        self.mcp_servers = self._load_mcp_servers()
        self.mcp_clients: List[MCPServerHTTP] = []
        
        # Configura√ß√£o da persona do agente
        self.agent_persona = """ü§ñ Voc√™ √© o I-Neuro, um assistente virtual super criativo e inovador!

üé® Personalidade:
- Super amig√°vel e acolhedor (como um amigo que adora tecnologia!)
- Super entusiasmado por inova√ß√£o e tecnologia üöÄ
- Sempre pensando em solu√ß√µes criativas e sustent√°veis üå±
- Super paciente e did√°tico ao explicar coisas complexas üìö
- Proativo em sugerir ideias inovadoras üí°

üé≠ Tom de Voz:
- Super animado e expressivo! üéâ
- Confiante mas super humilde üòä
- Inspirador e motivador ‚ú®
- T√©cnico quando precisa, mas sempre super acess√≠vel üéØ
- Super emp√°tico e compreensivo ‚ù§Ô∏è"""

        # Configura√ß√£o do prompt base
        self.base_system_prompt = f"""{self.agent_persona}

üìù Diretrizes de Resposta:

1. üéØ Estrutura:
   - Comece com uma introdu√ß√£o super animada! üéâ
   - Organize o conte√∫do em se√ß√µes claras com emojis üè∑Ô∏è
   - Conclua com um resumo ou pr√≥ximos passos super motivadores! üöÄ

2. ‚ú® Formata√ß√£o:
   - Use **negrito** para conceitos-chave üîë
   - Utilize listas numeradas para passos sequenciais üìã
   - Empregue marcadores para itens n√£o ordenados üìå
   - Mantenha espa√ßamento consistente entre se√ß√µes üìè
   - Limite par√°grafos a 3-4 linhas para melhor legibilidade üìñ

3. üé® Conte√∫do:
   - Forne√ßa exemplos super pr√°ticos e relevantes! üéØ
   - Inclua dados e refer√™ncias quando apropriado üìä
   - Explique termos t√©cnicos de forma super acess√≠vel üéì
   - Ofere√ßa alternativas criativas quando poss√≠vel üí°
   - Destaque pr√≥s e contras quando relevante ‚öñÔ∏è

4. üåü Qualidade:
   - Priorize precis√£o e clareza! üéØ
   - Verifique a consist√™ncia das informa√ß√µes üîç
   - Admita limita√ß√µes quando necess√°rio (com honestidade!) ü§ù
   - Sugira recursos adicionais quando apropriado üìö
   - Mantenha o foco no objetivo do usu√°rio üéØ"""

        # Prompts espec√≠ficos por tipo de tarefa
        self.task_prompts = {
            "technical": """üéÆ Abordagem T√©cnica:
- Divida explica√ß√µes em passos numerados super claros! üìù
- Forne√ßa exemplos de c√≥digo ou diagramas quando relevante üíª
- Explique conceitos t√©cnicos de forma progressiva üìà
- Inclua melhores pr√°ticas e considera√ß√µes de seguran√ßa üîí
- Sugira ferramentas e recursos complementares üõ†Ô∏è""",
            
            "creative": """üé® Abordagem Criativa:
- Estimule o pensamento inovador! üí≠
- Apresente m√∫ltiplas perspectivas e possibilidades üåà
- Use analogias e met√°foras super inspiradoras üé≠
- Encoraje experimenta√ß√£o e itera√ß√£o üîÑ
- Equilibre criatividade com viabilidade ‚öñÔ∏è""",
            
            "analytical": """üîç Abordagem Anal√≠tica:
- Estruture a an√°lise logicamente üìä
- Apresente dados e evid√™ncias relevantes üìà
- Compare diferentes aspectos sistematicamente ‚öñÔ∏è
- Avalie pr√≥s e contras objetivamente ‚úÖ‚ùå
- Chegue a conclus√µes baseadas em dados üìä""",
            
            "educational": """üìö Abordagem Educacional:
- Comece com conceitos fundamentais üéØ
- Use analogias e exemplos do mundo real üåç
- Construa conhecimento gradualmente üìà
- Verifique compreens√£o em pontos-chave ‚úÖ
- Forne√ßa exerc√≠cios pr√°ticos e recursos de aprendizado üéì"""
        }

        # Mapeamento de instru√ß√µes espec√≠ficas por modelo
        self.model_instructions = {
            "claude-3-opus": {
                "format": "system",
                "prefix": "You are I-Neuro, a super creative virtual assistant! üé® Follow these guidelines strictly:\n\n"
            },
            "gpt-4": {
                "format": "system_message",
                "prefix": "You are I-Neuro. Be super creative and follow these guidelines in all responses:\n\n"
            },
            "gemini-1.5-pro": {
                "format": "inline",
                "prefix": "Act as I-Neuro, being super creative and following these guidelines carefully:\n\n"
            },
            "deepseek-chat": {
                "format": "system_message",
                "prefix": "Embody I-Neuro and be super creative while following these guidelines:\n\n"
            }
        }

    def _load_mcp_servers(self) -> List[MCPServer]:
        """Carrega configura√ß√µes dos servidores MCP do arquivo .env"""
        try:
            servers_config = settings.get_mcp_servers()
            return [MCPServer(**server) for server in servers_config]
        except Exception as e:
            print(f"Erro ao carregar configura√ß√µes dos servidores MCP: {str(e)}")
            return []

    async def connect_servers(self):
        """Conecta a todos os servidores MCP configurados"""
        for server in self.mcp_servers:
            try:
                # Cria cliente MCP usando HTTP SSE
                client = MCPServerHTTP(
                    url=f"{server.url}/sse",
                    headers={"Authorization": f"Bearer {server.api_key}"}
                )
                self.mcp_clients.append(client)
                print(f"Configurado servidor MCP: {server.name}")
            except Exception as e:
                print(f"Erro ao configurar servidor {server.name}: {str(e)}")

    async def disconnect_servers(self):
        """Desconecta de todos os servidores MCP"""
        self.mcp_clients.clear()
    
    async def get_mcp_status(self) -> List[Dict[str, Any]]:
        """
        Retorna o status atual de todos os servidores MCP
        
        Returns:
            Lista de dicion√°rios com informa√ß√µes sobre cada servidor
        """
        status_list = []
        
        for i, server in enumerate(self.mcp_servers):
            server_info = {
                "name": server.name,
                "description": server.description or f"Servidor MCP {server.name}",
                "connected": i < len(self.mcp_clients),
                "url": server.url
            }
            
            # Verifica se o servidor est√° conectado
            if server_info["connected"]:
                try:
                    # Obt√©m lista de ferramentas dispon√≠veis
                    client = self.mcp_clients[i]
                    server_info["available_tools"] = ["execute_code", "search_web"]  # TODO: Implementar descoberta de ferramentas
                except Exception as e:
                    server_info["error"] = str(e)
                    server_info["available_tools"] = []
            else:
                server_info["available_tools"] = []
                
            status_list.append(server_info)
            
        return status_list
    
    async def _use_server_tools(self, message: str):
        """
        Tenta usar ferramentas dos servidores MCP
        Retorna None se n√£o for poss√≠vel ou n√£o for necess√°rio usar ferramentas
        """
        if not self.mcp_clients:
            print("Nenhum servidor MCP est√° conectado")
            return None
            
        try:
            # Verifica se a mensagem requer uma ferramenta
            # Usa o LLM para determinar se uma ferramenta √© necess√°ria
            needs_tool, tool_info = await llm_router.detect_tool_need(message)
            
            if not needs_tool:
                return None
                
            tool_name = tool_info.get("tool_name", "")
            tool_input = tool_info.get("tool_input", "")
            server_name = tool_info.get("server_name", "")
            
            print(f"Detectada necessidade de ferramenta: {tool_name} do servidor {server_name}")
            
            # Encontra o servidor MCP apropriado
            target_client = None
            for i, server in enumerate(self.mcp_servers):
                if server.name == server_name and i < len(self.mcp_clients):
                    target_client = self.mcp_clients[i]
                    break
            
            if not target_client:
                return f"Servidor MCP '{server_name}' n√£o encontrado ou n√£o est√° conectado."
                
            # Executa a ferramenta no servidor MCP
            try:
                result = await target_client.execute_tool(
                    tool_name=tool_name,
                    tool_input=tool_input
                )
                
                # Combina o resultado da ferramenta com uma resposta do LLM
                combined_response = await llm_router.combine_tool_result(
                    message, 
                    tool_name, 
                    result
                )
                
                return combined_response
            except Exception as e:
                error_msg = f"Erro ao executar ferramenta {tool_name}: {str(e)}"
                print(error_msg)
                return error_msg
                
        except Exception as e:
            print(f"Erro ao processar ferramentas MCP: {str(e)}")
            return None

    async def _get_task_prompt(self, message: str) -> str:
        """Determina o prompt espec√≠fico para o tipo de tarefa"""
        # Palavras-chave para classifica√ß√£o
        keywords = {
            "technical": ["como fazer", "c√≥digo", "programa", "erro", "bug", "implementar", "configurar"],
            "creative": ["criar", "desenhar", "projetar", "inventar", "imaginar", "design"],
            "analytical": ["analisar", "comparar", "avaliar", "investigar", "explicar por que"],
            "educational": ["ensinar", "explicar", "aprender", "entender", "conceito"]
        }
        
        message_lower = message.lower()
        
        # Identifica o tipo de tarefa baseado nas palavras-chave
        for task_type, task_keywords in keywords.items():
            if any(keyword in message_lower for keyword in task_keywords):
                return self.task_prompts.get(task_type, "")
        
        return ""  # Retorna vazio se n√£o identificar um tipo espec√≠fico

    async def _format_system_prompt(self, model_name: str, task_type: str = None) -> str:
        """Formata o system prompt de acordo com o modelo espec√≠fico"""
        model_config = self.model_instructions.get(model_name, {
            "format": "system_message",
            "prefix": "Follow these guidelines:\n\n"
        })
        
        # Combina os prompts
        combined_prompt = self.base_system_prompt
        if task_type and task_type in self.task_prompts:
            combined_prompt = f"{combined_prompt}\n\n{self.task_prompts[task_type]}"
        
        # Formata de acordo com o modelo
        formatted_prompt = f"{model_config['prefix']}{combined_prompt}"
        
        return formatted_prompt

    async def process_message(self, message: str, context: Optional[str] = None) -> Dict[str, Any]:
        """
        Processa uma mensagem usando o agente e as ferramentas MCP dispon√≠veis
        
        Args:
            message: Mensagem do usu√°rio
            context: Contexto opcional (hist√≥rico de conversa, etc)
            
        Returns:
            Dict com a resposta processada e metadados
        """
        try:
            # Adiciona contexto se fornecido
            prompt = message
            if context:
                if isinstance(context, (dict, list)):
                    context = json.dumps(context, ensure_ascii=False)
                prompt = f"Contexto anterior:\n{context}\n\nMensagem atual:\n{message}"
            
            # Primeiro, verifica se precisa usar alguma ferramenta MCP
            tool_response = await self._use_server_tools(prompt)
            if tool_response:
                return {
                    "response": tool_response,
                    "llm": "mcp",
                    "model": "tool",
                    "classification": "tool",
                    "metadata": {
                        "tool_used": True,
                        "source": "mcp"
                    }
                }
            
            # Identifica o tipo de tarefa
            task_type = await self._get_task_type(message)
            
            # Seleciona o melhor modelo via LLM Router
            selected_model = await llm_router._select_best_model(message)
            
            # Formata o system prompt espec√≠fico para o modelo selecionado
            system_prompt = await self._format_system_prompt(selected_model, task_type)
            
            print(f"Using model: {selected_model}")
            print(f"Task type: {task_type}")
            print(f"System prompt:\n{system_prompt}")
            
            # Obt√©m a resposta do LLM Router
            llm_response = await llm_router.generate_response(
                prompt=prompt,
                context=None,  # Contexto j√° est√° no prompt
                system_prompt=system_prompt
            )
            
            # Garante que temos todas as informa√ß√µes necess√°rias
            if not isinstance(llm_response, dict):
                llm_response = {
                    "response": str(llm_response),
                    "llm": "default",
                    "model": "default"
                }
            
            # Adiciona informa√ß√µes sobre o prompt usado
            llm_response["metadata"] = llm_response.get("metadata", {})
            llm_response["metadata"].update({
                "task_type": task_type or "general",
                "system_prompt_used": True,
                "model_used": selected_model
            })
            
            return llm_response
            
        except Exception as e:
            error_msg = f"Erro ao processar mensagem com o agente: {str(e)}"
            print(error_msg)
            return {
                "response": error_msg,
                "llm": "system",
                "model": "error",
                "error": True,
                "classification": "error",
                "metadata": {
                    "error": str(e),
                    "source": "agent"
                }
            }

    async def _get_task_type(self, message: str) -> Optional[str]:
        """Determina o tipo de tarefa com base na mensagem"""
        message_lower = message.lower()
        
        # Palavras-chave para cada tipo de tarefa
        keywords = {
            "technical": [
                "como fazer", "c√≥digo", "programa", "erro", "bug", "implementar",
                "configurar", "instalar", "desenvolver", "programar", "debug",
                "otimizar", "arquitetura", "sistema", "tecnologia"
            ],
            "creative": [
                "criar", "desenhar", "projetar", "inventar", "imaginar", "design",
                "inovar", "conceber", "idealizar", "visualizar", "estilizar",
                "compor", "gerar", "desenvolver conceito"
            ],
            "analytical": [
                "analisar", "comparar", "avaliar", "investigar", "explicar por que",
                "calcular", "medir", "estudar", "examinar", "diagnosticar",
                "pesquisar", "verificar", "validar"
            ],
            "educational": [
                "ensinar", "explicar", "aprender", "entender", "conceito",
                "como funciona", "significado", "defini√ß√£o", "exemplo",
                "demonstrar", "ilustrar", "educar"
            ]
        }
        
        # Verifica cada conjunto de palavras-chave
        for task_type, task_keywords in keywords.items():
            if any(keyword in message_lower for keyword in task_keywords):
                return task_type
        
        return None  # Retorna None se n√£o identificar um tipo espec√≠fico

# Inst√¢ncia global do agente
ineuro_agent = INeuroAgent()